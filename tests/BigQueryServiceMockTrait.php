<?php
namespace App\Tests;

use App\Interfaces\BigQueryServiceInterface;

trait BigQueryServiceMockTrait
{
    private function configBigQueryServiceMock()
    {
        $bigQueryService = $this->createMock(BigQueryServiceInterface::class);

        $bigQueryService
            ->method('getPostList')
            ->willReturn(
                json_decode('[{"id":73842189,"title":"Need to map JSON file fields and store it in a JSON file to upload it in BQ table but it\u0027s not working","body":"\u003Cp\u003EI have a DAG which can insert data of JSON file in BQ table but I have a JSON file and many fields are started with illegal characters. So I have created BQ table with legal column names but I need to map BQ column names with JSON file fields and store it in a JSON file, to upload it into BQ table but it\u0027s not working.\u003C\/p\u003E\n\u003Cp\u003EMy JSON file :-\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E{\u0026quot;ID\u0026quot;:\u0026quot;4238382\u0026quot;,\u0026quot;Title\u0026quot;:\u0026quot;El clon Cap\\u00edtulo 3\u0026quot;,\u0026quot;Description\u0026quot;:\u0026quot;Said y Ali llegan a un acuerdo. Leonidas sale con Yvete y Diogo. Edvaldo no quiere hacerse los ex\\u00e1menes. Jade se reh\\u00fasa a usar velo. Lucas se disculpa con Ali. Albieri dice que Ali fue duro con Jade, Ali lo acusa de querer experimentar con humanos.\u0026quot;,\u0026quot;Program\u0026quot;:\u0026quot;El Clon\u0026quot;,\u0026quot;Season\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;Episode\u0026quot;:\u0026quot;3\u0026quot;,\u0026quot;Source\u0026quot;:\u0026quot;GLOBO TV INTERNACIONAL\u0026quot;,\u0026quot;Category\u0026quot;:\u0026quot;Drama\u0026quot;,\u0026quot;Syndicator\u0026quot;:\u0026quot;CSv2\u0026quot;,\u0026quot;[CSv2] external_id\u0026quot;:\u0026quot;ELCL100002002\u0026quot;,\u0026quot;[CSv2] pub_win_US_begin\u0026quot;:\u0026quot;1661842800\u0026quot;,\u0026quot;[CSv2] pub_win_US_end\u0026quot;:\u0026quot;1754625600\u0026quot;,\u0026quot;[CSv2] language\u0026quot;:\u0026quot;es\u0026quot;,\u0026quot;[CSv2] title\u0026quot;:\u0026quot;El clon Cap\\u00edtulo 3\u0026quot;,\u0026quot;[CSv2] descriptive_title\u0026quot;:\u0026quot;Acuerdo de matrimonio\u0026quot;,\u0026quot;[CSv2] description\u0026quot;:\u0026quot;Said y Ali llegan a un acuerdo. Leonidas sale con Yvete y Diogo. Edvaldo no quiere hacerse los ex\\u00e1menes. Jade se reh\\u00fasa a usar velo. Lucas se disculpa con Ali. Albieri dice que Ali fue duro con Jade, Ali lo acusa de querer experimentar con humanos.\u0026quot;,\u0026quot;[CSv2] supplier\u0026quot;:\u0026quot;GLOBO TV INTERNACIONAL\u0026quot;,\u0026quot;[CSv2] categories\u0026quot;:\u0026quot;Drama\u0026quot;,\u0026quot;[CSv2] rating\u0026quot;:\u0026quot;TV-14\u0026quot;,\u0026quot;[CSv2] subratings\u0026quot;:\u0026quot;D\u0026quot;,\u0026quot;[CSv2] program_type\u0026quot;:\u0026quot;NOVELA\u0026quot;,\u0026quot;[CSv2] entity\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;[CSv2] exception_countries\u0026quot;:\u0026quot;US ,\\tUM ,PR ,\\tMX ,\\tAR ,\\tCL ,\\tCO ,\\tPE ,\\tEC ,\\tCR ,\\tSV ,\\tHN ,\\tBO ,\\tPA ,\\tDO ,\\tNI ,\\tPY ,\\tVE ,\\tUY ,\\tGT\u0026quot;,\u0026quot;[CSv2] episode_type\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;TMS ID\u0026quot;:null,\u0026quot;external_id\u0026quot;:\u0026quot;ELCL100002002\u0026quot;,\u0026quot;Content Type\u0026quot;:\u0026quot;Entertainment\u0026quot;,\u0026quot;Release Year\u0026quot;:\u0026quot;2001\u0026quot;,\u0026quot;sports_event_ID\u0026quot;:\u0026quot;\u0026quot;}\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EMy BQ table :-\u003C\/p\u003E\n\u003Cp\u003E\u003Ca href=\u0022https:\/\/i.stack.imgur.com\/6iI7h.png\u0022 rel=\u0022nofollow noreferrer\u0022\u003E\u003Cimg src=\u0022https:\/\/i.stack.imgur.com\/6iI7h.png\u0022 alt=\u0022enter image description here\u0022 \/\u003E\u003C\/a\u003E\u003C\/p\u003E\n\u003Cp\u003ECreated a python plugin file to map the fields of JSON file with BQ columns and store it in BQ :-\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003Eimport json\nimport csv\nimport logging\nimport os\nimport bson.json_util as json_util\n\nfrom google.cloud import storage\nfrom pydantic import BaseModel, Field, validator\n\n\nclass EventsModel(BaseModel):\n    ID: int = None\n    Title: str = None\n    Description: str = None\n    Program: str = None\n    Season: int = None\n    Episode: int = None\n    Source: str = None\n    Category: str = None\n    Syndicator: str = None\n    CSv2_external_id: str = Field(alias=\u0026quot;[CSv2] external_id\u0026quot;, default=None)\n    CSv2_pub_win_US_begin: int = Field(alias=\u0026quot;[CSv2] pub_win_US_begin\u0026quot;, default=None)\n    CSv2_pub_win_US_end: int    = Field(alias=\u0026quot;[CSv2] pub_win_US_end\u0026quot;, default=None)\n    CSv2_language: str = Field(alias=\u0026quot;[CSv2] language\u0026quot;, default=None)\n    CSv2_title: str = Field(alias=\u0026quot;[CSv2] title\u0026quot;, default=None)\n    CSv2_descriptive_title: str = Field(alias=\u0026quot;[CSv2] descriptive_title\u0026quot;, default=None)\n    CSv2_description: str = Field(alias=\u0026quot;[CSv2] description\u0026quot;, default=None)\n    CSv2_supplier: str = Field(alias=\u0026quot;[CSv2] supplier\u0026quot;, default=None)\n    CSv2_categories: str = Field(alias=\u0026quot;[CSv2] categories\u0026quot;, default=None)\n    CSv2_rating: str = Field(alias=\u0026quot;[CSv2] rating\u0026quot;, default=None)\n    CSv2_subratings: str = Field(alias=\u0026quot;[CSv2] subratings\u0026quot;, default=None)\n    CSv2_program_type: str = Field(alias=\u0026quot;[CSv2] program_type\u0026quot;, default=None)\n    CSv2_entity: str = Field(alias=\u0026quot;[CSv2] entity\u0026quot;, default=None)\n    CSv2_exception_countries: str = Field(alias=\u0026quot;[CSv2] exception_countries\u0026quot;, default=None)\n    CSv2_episode_type: str = Field(alias=\u0026quot;[CSv2] episode_type\u0026quot;, default=None)\n    TMS_ID: str = Field(alias=\u0026quot;TMS ID\u0026quot;, default= None)\n    external_id: str = None\n    Content_Type: str = None\n    Release_Year: int = None\n    sports_event_ID: str = None\n\n    @validator(\n        \u0026quot;TMS_ID\u0026quot;,\n        pre=True,\n        always=True,\n    )\n    def is_date(cls, v):\n        try:\n            if type(v) == str:\n                v = None if v.lower() ==\u0026quot;null\u0026quot; else v\n            else:\n                raise ValueError\n        except ValueError:\n            v = \u0026quot;null\u0026quot;\n        return v\n\n\ndef map_keys(bucket_name, file_path, list_of_files): #pass the folder as an argument\n            \n    logging.info(f\u0026quot;bucket_name: {bucket_name}\u0026quot;)\n    logging.info(f\u0026quot;file_path: {file_path}\u0026quot;)\n\n    storage_client = storage.Client()\n\n    \n    path = f\u0027\u0027\u0027{bucket_name}\u0027\u0027\u0027\n    \n\n    logging.info(f\u0026quot;list_of_files from the DAG: {list_of_files}\u0026quot;)\n    blobs  = storage_client.list_blobs(\n    bucket_or_name=path\n    )\n\n    file = \u0026quot;\u0026quot;\n    logging.info(f\u0026quot;blob {blobs}\u0026quot;)\n\n    for blob in blobs:\n        if not blob.name.endswith(\u0026quot;\/\u0026quot;):\n            file = blob.name\n            bucket = storage_client.get_bucket(bucket_name)\n        \n        #TODO: iterate the files into the path and parse using the new model\n        \n        logging.info(f\u0026quot;file: {file}\u0026quot;)\n\n        \n        with open(file, \u0026quot;w\u0026quot;) as j_file:\n                    j_file.write(json_util.dumps(file))\n                    j_file.write(\u0026quot;\\n\u0026quot;)\n        # mapper del modelo\n        new_model = EventsModel.parse_obj(j_file)\n        new_model = new_model.dict()\n\n        with open(new_model, \u0026quot;w\u0026quot;) as file_transformed:\n            file_transformed.write(json.dumps(new_model))\n            file_transformed.write(\u0026quot;\\n\u0026quot;)\n        \n        blob = bucket.blob(f\u0026quot;test_file\u0026quot;)\n        blob.upload_from_filename(file_transformed)\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EAdded it in DAG :-\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003Emap_json_keys_with_BQ_columns = PythonOperator(\n        task_id=\u0026quot;map_json_keys_with_BQ_columns\u0026quot;,\n        retries=0,\n        python_callable=map_keys,\n        op_kwargs={\n            \u0026quot;bucket_name\u0026quot;: mcp_bucket,\n            \u0026quot;file_path\u0026quot;: mcp_source_folder,\n            \u0026quot;list_of_files\u0026quot;: source_files\n        },\n        dag=dag,\n    )\n\n\n    mcp_ingestion_to_bq = GCSToBigQueryOperator(\n        task_id=\u0026quot;mcp_ingestion_to_bq\u0026quot;,\n        retries=0,\n        dag=dag,\n        bucket=mcp_bucket,\n        source_objects=f\u0026quot;{mcp_source_folder}*.json\u0026quot;,\n        source_format=\u0026quot;NEWLINE_DELIMITED_JSON\u0026quot;,\n        #skip_leading_rows=16,\n        destination_project_dataset_table=destination_bq_table,\n        write_disposition=\u0026quot;WRITE_TRUNCATE\u0026quot;,\n        create_disposition=\u0026quot;CREATE_NEVER\u0026quot;,\n        \n        autodetect=True\n    )\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EBut it\u0027s not working. Getting an error :-\n\u003Ca href=\u0022https:\/\/i.stack.imgur.com\/KmZI4.png\u0022 rel=\u0022nofollow noreferrer\u0022\u003E\u003Cimg src=\u0022https:\/\/i.stack.imgur.com\/KmZI4.png\u0022 alt=\u0022enter image description here\u0022 \/\u003E\u003C\/a\u003E\nError :-\n\u003Ca href=\u0022https:\/\/i.stack.imgur.com\/RmK37.png\u0022 rel=\u0022nofollow noreferrer\u0022\u003E\u003Cimg src=\u0022https:\/\/i.stack.imgur.com\/RmK37.png\u0022 alt=\u0022enter image description here\u0022 \/\u003E\u003C\/a\u003E\u003C\/p\u003E","accepted_answer_id":null,"answer_count":0,"comment_count":0,"community_owned_date":null,"creation_date":{},"favorite_count":null,"last_activity_date":{},"last_edit_date":null,"last_editor_display_name":null,"last_editor_user_id":null,"owner_display_name":null,"owner_user_id":19234248,"parent_id":null,"post_type_id":1,"score":0,"tags":"python|google-cloud-platform|google-bigquery|airflow|google-cloud-composer","view_count":6},{"id":73840853,"title":"Big Query Multiple Independent Arrays of Structures","body":"\u003Cp\u003EI am trying to create two arrays of structures that are independent of each other.\u003C\/p\u003E\n\u003Cp\u003EI am trying to get these arrays created from another table that is an import of a flat file with different record types depending on the data (Rec0015 - Earnings and Rec0025 - Deductions). Each record type has a slightly different layout, so I import the record data as a string that is \u0026quot;~\u0026quot; delimited. I have simplified my examples below to only show the basic data needed to illustrate my issues.\u003C\/p\u003E\n\u003Cp\u003EIf I run the queries independently of each other each array is created properly; however, when I combine both I cannot get it to work.\u003C\/p\u003E\n\u003Cp\u003EFor the first query:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003ESELECT \n     flat.EmployeeNumber,\n     ARRAY_AGG(STRUCT(\n          flat.RecordNumberEarn,\n          flat.EmployeeEarningsAmount\n     ))\n     as EmployeeEarningsDetail\nFROM \n(SELECT DISTINCT\n     stage.EmployeeNumber,\n     Rec0015.RecordNumberEarn,\n     Rec0015.EmployeeEarningsAmount,\nFROM  bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging stage\nINNER JOIN \n(SELECT EmployeeNumber,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(0)] AS RecordNumberEarn,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(4)] AS EmployeeEarningsAmount\n FROM bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging\n WHERE RecordTypeNumber = \u00270015\u0027\n) Rec0015\nON stage.EmployeeNumber = Rec0015.EmployeeNumber  \n) as flat \nGROUP BY\n     flat.EmployeeNumber\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EI get the following result (which is correct):\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003EEmployeeNumber  EmployeeEarningsDetail\nxxxx521     \u0026quot;{\n  \u0026quot;\u0026quot;EmployeeEarningsDetail\u0026quot;\u0026quot;: [{\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }]\n}\u0026quot; ....\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EWhen I run the \u0026quot;full\u0026quot; query:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003ESELECT \n     flat2.EmployeeNumber,\n     ARRAY_AGG(STRUCT(\n          Rec0025.RecordNumberDed,\n          Rec0025.EmployeePayDeductionAmount\n     ))\n     as EmployeeDeductionsDetail\nFROM \n\n(SELECT \n     flat.EmployeeNumber,\n     ARRAY_AGG(STRUCT(\n          flat.RecordNumberEarn,\n          flat.EmployeeEarningsAmount\n     ))\n     as EmployeeEarningsDetail\nFROM \n(SELECT DISTINCT\n     stage.EmployeeNumber,\n     Rec0015.RecordNumberEarn,\n     Rec0015.EmployeeEarningsAmount,\nFROM  bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging stage\nINNER JOIN \n(SELECT EmployeeNumber,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(0)] AS RecordNumberEarn,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(4)] AS EmployeeEarningsAmount\n FROM bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging\n WHERE RecordTypeNumber = \u00270015\u0027\n) Rec0015\nON stage.EmployeeNumber = Rec0015.EmployeeNumber  \n) as flat \nGROUP BY\n     flat.EmployeeNumber\n) as flat2\nINNER JOIN \n(SELECT DISTINCT EmployeeNumber,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(0)] AS RecordNumberDed,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(5)] AS EmployeePayDeductionAmount\n FROM bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging\n WHERE RecordTypeNumber = \u00270025\u0027\n) Rec0025 \nON flat2.EmployeeNumber = Rec0025.EmployeeNumber\nGROUP BY\n     flat2.EmployeeNumber\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EI get the following result (which is correct, but doesn\u0027t include the first array structure):\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003EEmployeeNumber  EmployeeDeductionsDetail\nxxxx521     \u0026quot;{\n  \u0026quot;\u0026quot;EmployeeDeductionsDetail\u0026quot;\u0026quot;: [{\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       50.65\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0003\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0004\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0005\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0006\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       61.54\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0007\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       13.22\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0008\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        7.84\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0009\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        0.69\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0010\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        5.00\u0026quot;\u0026quot;\n  }]\n}\u0026quot; ...\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EHowever, in the 2nd query, I really want to do a group by on the array structure EmployeeEarningsDetail, but when I add the array to the select and group by I get the error:\u003C\/p\u003E\n\u003Cp\u003E\u0026quot;Grouping by expressions of type ARRAY is not allowed.\u0026quot;\u003C\/p\u003E\n\u003Cp\u003EI tried adding a TO_JSON_STRING(EmployeeEarningsDetail) in both the Select and the Group by, but I got a column of just the string not as an array as below:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003ESELECT \n     flat2.EmployeeNumber,\n     TO_JSON_STRING(flat2.EmployeeEarningsDetail),\n     ARRAY_AGG(STRUCT(\n          Rec0025.RecordNumberDed,\n          Rec0025.EmployeePayDeductionAmount\n     ))\n     as EmployeeDeductionsDetail\nFROM \n\n(SELECT \n     flat.EmployeeNumber,\n     ARRAY_AGG(STRUCT(\n          flat.RecordNumberEarn,\n          flat.EmployeeEarningsAmount\n     ))\n     as EmployeeEarningsDetail\nFROM \n(SELECT DISTINCT\n     stage.EmployeeNumber,\n     Rec0015.RecordNumberEarn,\n     Rec0015.EmployeeEarningsAmount,\nFROM  bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging stage\nINNER JOIN \n(SELECT EmployeeNumber,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(0)] AS RecordNumberEarn,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(4)] AS EmployeeEarningsAmount\n FROM bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging\n WHERE RecordTypeNumber = \u00270015\u0027\n) Rec0015\nON stage.EmployeeNumber = Rec0015.EmployeeNumber  \n) as flat \nGROUP BY\n     flat.EmployeeNumber\n) as flat2\nINNER JOIN \n(SELECT DISTINCT EmployeeNumber,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(0)] AS RecordNumberDed,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(5)] AS EmployeePayDeductionAmount\n FROM bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging\n WHERE RecordTypeNumber = \u00270025\u0027\n) Rec0025 \nON flat2.EmployeeNumber = Rec0025.EmployeeNumber\nGROUP BY\n     flat2.EmployeeNumber,\n     TO_JSON_STRING(flat2.EmployeeEarningsDetail)\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EThe results were (not correct at all) are below the JSON shows an f0_ string of all the earnings and the first Deduction followed by another row with the rest of the deductions in an array See below:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003EEmployeeNumber  f0_ EmployeeDeductionsDetail\nxxxx521     \u0026quot;[{\u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;:\u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;:\u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;},{\u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;:\u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;:\u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;}]\u0026quot; \u0026quot;{\n  \u0026quot;\u0026quot;EmployeeDeductionsDetail\u0026quot;\u0026quot;: [{\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       50.65\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0003\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0004\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0005\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0006\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       61.54\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0007\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       13.22\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0008\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        7.84\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0009\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        0.69\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0010\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        5.00\u0026quot;\u0026quot;\n  }]\n}\u0026quot; ...\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EI have tried to put both ARRAY_AGGs in the same SELECT my arrays are cartesian products:\u003C\/p\u003E\n\u003Cp\u003EThe SQL is:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003ESELECT \n     flat.EmployeeNumber,\n     ARRAY_AGG(STRUCT(\n          flat.RecordNumberEarn,\n          flat.EmployeeEarningsAmount\n     ))\n     as EmployeeEarningsDetail,\n     ARRAY_AGG(STRUCT(\n          flat.RecordNumberDed,\n          flat.EmployeePayDeductionAmount\n     ))\n     as EmployeeDeductionsDetail\nFROM \n(SELECT DISTINCT\n     stage.EmployeeNumber,\n     Rec0015.RecordNumberEarn,\n     Rec0015.EmployeeEarningsAmount,\n     Rec0025.RecordNumberDed,\n     Rec0025.EmployeePayDeductionAmount\nFROM  bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging stage\nINNER JOIN \n(SELECT EmployeeNumber,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(0)] AS RecordNumberEarn,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(4)] AS EmployeeEarningsAmount\n FROM bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging\n WHERE RecordTypeNumber = \u00270015\u0027\n) Rec0015\nON stage.EmployeeNumber = Rec0015.EmployeeNumber  \nINNER JOIN \n(SELECT DISTINCT EmployeeNumber,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(0)] AS RecordNumberDed,\n SPLIT(VariableData, \u0027~\u0027)[SAFE_OFFSET(5)] AS EmployeePayDeductionAmount\n FROM bq_hpm_ppm_dev.EmployeeGrossToNetPayStaging\n WHERE RecordTypeNumber = \u00270025\u0027\n) Rec0025 \nON stage.EmployeeNumber = Rec0025.EmployeeNumber\n) as flat\nGROUP BY\n     flat.EmployeeNumber\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003Eand the results are:\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003EEmployeeNumber  EmployeeEarningsDetail  EmployeeDeductionsDetail\nxxxx521     \u0026quot;{\n  \u0026quot;\u0026quot;EmployeeEarningsDetail\u0026quot;\u0026quot;: [{\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      375.52\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberEarn\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeeEarningsAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;      387.26\u0026quot;\u0026quot;\n  }]\n}\u0026quot;  \u0026quot;{\n  \u0026quot;\u0026quot;EmployeeDeductionsDetail\u0026quot;\u0026quot;: [{\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       50.65\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0003\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0004\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0005\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0006\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       61.54\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0007\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       13.22\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0008\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        7.84\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0009\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        0.69\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0010\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        5.00\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0001\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       50.65\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0002\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0003\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       44.15\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0004\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0005\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       10.33\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0006\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       61.54\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0007\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;       13.22\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0008\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        7.84\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0009\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        0.69\u0026quot;\u0026quot;\n  }, {\n    \u0026quot;\u0026quot;RecordNumberDed\u0026quot;\u0026quot;: \u0026quot;\u0026quot;0010\u0026quot;\u0026quot;,\n    \u0026quot;\u0026quot;EmployeePayDeductionAmount\u0026quot;\u0026quot;: \u0026quot;\u0026quot;        5.00\u0026quot;\u0026quot;\n  }]\n}\u0026quot; ...\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EAny suggestions on how I can \u0026quot;fix\u0026quot; this.\u003C\/p\u003E\n\u003Cp\u003EThanks,\nDavid\u003C\/p\u003E","accepted_answer_id":null,"answer_count":1,"comment_count":3,"community_owned_date":null,"creation_date":{},"favorite_count":null,"last_activity_date":{},"last_edit_date":null,"last_editor_display_name":null,"last_editor_user_id":null,"owner_display_name":null,"owner_user_id":19346866,"parent_id":null,"post_type_id":1,"score":0,"tags":"arrays|google-bigquery|structure","view_count":17}]')
            )
        ;

        $bigQueryService
            ->method('getPost')
            ->willReturn(
                json_decode('[{"id":73842189,"title":"NAUZET  GET POST Need to map JSON file fields and store it in a JSON file to upload it in BQ table but it\u0027s not working","body":"\u003Cp\u003EI have a DAG which can insert data of JSON file in BQ table but I have a JSON file and many fields are started with illegal characters. So I have created BQ table with legal column names but I need to map BQ column names with JSON file fields and store it in a JSON file, to upload it into BQ table but it\u0027s not working.\u003C\/p\u003E\n\u003Cp\u003EMy JSON file :-\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003E{\u0026quot;ID\u0026quot;:\u0026quot;4238382\u0026quot;,\u0026quot;Title\u0026quot;:\u0026quot;El clon Cap\\u00edtulo 3\u0026quot;,\u0026quot;Description\u0026quot;:\u0026quot;Said y Ali llegan a un acuerdo. Leonidas sale con Yvete y Diogo. Edvaldo no quiere hacerse los ex\\u00e1menes. Jade se reh\\u00fasa a usar velo. Lucas se disculpa con Ali. Albieri dice que Ali fue duro con Jade, Ali lo acusa de querer experimentar con humanos.\u0026quot;,\u0026quot;Program\u0026quot;:\u0026quot;El Clon\u0026quot;,\u0026quot;Season\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;Episode\u0026quot;:\u0026quot;3\u0026quot;,\u0026quot;Source\u0026quot;:\u0026quot;GLOBO TV INTERNACIONAL\u0026quot;,\u0026quot;Category\u0026quot;:\u0026quot;Drama\u0026quot;,\u0026quot;Syndicator\u0026quot;:\u0026quot;CSv2\u0026quot;,\u0026quot;[CSv2] external_id\u0026quot;:\u0026quot;ELCL100002002\u0026quot;,\u0026quot;[CSv2] pub_win_US_begin\u0026quot;:\u0026quot;1661842800\u0026quot;,\u0026quot;[CSv2] pub_win_US_end\u0026quot;:\u0026quot;1754625600\u0026quot;,\u0026quot;[CSv2] language\u0026quot;:\u0026quot;es\u0026quot;,\u0026quot;[CSv2] title\u0026quot;:\u0026quot;El clon Cap\\u00edtulo 3\u0026quot;,\u0026quot;[CSv2] descriptive_title\u0026quot;:\u0026quot;Acuerdo de matrimonio\u0026quot;,\u0026quot;[CSv2] description\u0026quot;:\u0026quot;Said y Ali llegan a un acuerdo. Leonidas sale con Yvete y Diogo. Edvaldo no quiere hacerse los ex\\u00e1menes. Jade se reh\\u00fasa a usar velo. Lucas se disculpa con Ali. Albieri dice que Ali fue duro con Jade, Ali lo acusa de querer experimentar con humanos.\u0026quot;,\u0026quot;[CSv2] supplier\u0026quot;:\u0026quot;GLOBO TV INTERNACIONAL\u0026quot;,\u0026quot;[CSv2] categories\u0026quot;:\u0026quot;Drama\u0026quot;,\u0026quot;[CSv2] rating\u0026quot;:\u0026quot;TV-14\u0026quot;,\u0026quot;[CSv2] subratings\u0026quot;:\u0026quot;D\u0026quot;,\u0026quot;[CSv2] program_type\u0026quot;:\u0026quot;NOVELA\u0026quot;,\u0026quot;[CSv2] entity\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;[CSv2] exception_countries\u0026quot;:\u0026quot;US ,\\tUM ,PR ,\\tMX ,\\tAR ,\\tCL ,\\tCO ,\\tPE ,\\tEC ,\\tCR ,\\tSV ,\\tHN ,\\tBO ,\\tPA ,\\tDO ,\\tNI ,\\tPY ,\\tVE ,\\tUY ,\\tGT\u0026quot;,\u0026quot;[CSv2] episode_type\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;TMS ID\u0026quot;:null,\u0026quot;external_id\u0026quot;:\u0026quot;ELCL100002002\u0026quot;,\u0026quot;Content Type\u0026quot;:\u0026quot;Entertainment\u0026quot;,\u0026quot;Release Year\u0026quot;:\u0026quot;2001\u0026quot;,\u0026quot;sports_event_ID\u0026quot;:\u0026quot;\u0026quot;}\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EMy BQ table :-\u003C\/p\u003E\n\u003Cp\u003E\u003Ca href=\u0022https:\/\/i.stack.imgur.com\/6iI7h.png\u0022 rel=\u0022nofollow noreferrer\u0022\u003E\u003Cimg src=\u0022https:\/\/i.stack.imgur.com\/6iI7h.png\u0022 alt=\u0022enter image description here\u0022 \/\u003E\u003C\/a\u003E\u003C\/p\u003E\n\u003Cp\u003ECreated a python plugin file to map the fields of JSON file with BQ columns and store it in BQ :-\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003Eimport json\nimport csv\nimport logging\nimport os\nimport bson.json_util as json_util\n\nfrom google.cloud import storage\nfrom pydantic import BaseModel, Field, validator\n\n\nclass EventsModel(BaseModel):\n    ID: int = None\n    Title: str = None\n    Description: str = None\n    Program: str = None\n    Season: int = None\n    Episode: int = None\n    Source: str = None\n    Category: str = None\n    Syndicator: str = None\n    CSv2_external_id: str = Field(alias=\u0026quot;[CSv2] external_id\u0026quot;, default=None)\n    CSv2_pub_win_US_begin: int = Field(alias=\u0026quot;[CSv2] pub_win_US_begin\u0026quot;, default=None)\n    CSv2_pub_win_US_end: int    = Field(alias=\u0026quot;[CSv2] pub_win_US_end\u0026quot;, default=None)\n    CSv2_language: str = Field(alias=\u0026quot;[CSv2] language\u0026quot;, default=None)\n    CSv2_title: str = Field(alias=\u0026quot;[CSv2] title\u0026quot;, default=None)\n    CSv2_descriptive_title: str = Field(alias=\u0026quot;[CSv2] descriptive_title\u0026quot;, default=None)\n    CSv2_description: str = Field(alias=\u0026quot;[CSv2] description\u0026quot;, default=None)\n    CSv2_supplier: str = Field(alias=\u0026quot;[CSv2] supplier\u0026quot;, default=None)\n    CSv2_categories: str = Field(alias=\u0026quot;[CSv2] categories\u0026quot;, default=None)\n    CSv2_rating: str = Field(alias=\u0026quot;[CSv2] rating\u0026quot;, default=None)\n    CSv2_subratings: str = Field(alias=\u0026quot;[CSv2] subratings\u0026quot;, default=None)\n    CSv2_program_type: str = Field(alias=\u0026quot;[CSv2] program_type\u0026quot;, default=None)\n    CSv2_entity: str = Field(alias=\u0026quot;[CSv2] entity\u0026quot;, default=None)\n    CSv2_exception_countries: str = Field(alias=\u0026quot;[CSv2] exception_countries\u0026quot;, default=None)\n    CSv2_episode_type: str = Field(alias=\u0026quot;[CSv2] episode_type\u0026quot;, default=None)\n    TMS_ID: str = Field(alias=\u0026quot;TMS ID\u0026quot;, default= None)\n    external_id: str = None\n    Content_Type: str = None\n    Release_Year: int = None\n    sports_event_ID: str = None\n\n    @validator(\n        \u0026quot;TMS_ID\u0026quot;,\n        pre=True,\n        always=True,\n    )\n    def is_date(cls, v):\n        try:\n            if type(v) == str:\n                v = None if v.lower() ==\u0026quot;null\u0026quot; else v\n            else:\n                raise ValueError\n        except ValueError:\n            v = \u0026quot;null\u0026quot;\n        return v\n\n\ndef map_keys(bucket_name, file_path, list_of_files): #pass the folder as an argument\n            \n    logging.info(f\u0026quot;bucket_name: {bucket_name}\u0026quot;)\n    logging.info(f\u0026quot;file_path: {file_path}\u0026quot;)\n\n    storage_client = storage.Client()\n\n    \n    path = f\u0027\u0027\u0027{bucket_name}\u0027\u0027\u0027\n    \n\n    logging.info(f\u0026quot;list_of_files from the DAG: {list_of_files}\u0026quot;)\n    blobs  = storage_client.list_blobs(\n    bucket_or_name=path\n    )\n\n    file = \u0026quot;\u0026quot;\n    logging.info(f\u0026quot;blob {blobs}\u0026quot;)\n\n    for blob in blobs:\n        if not blob.name.endswith(\u0026quot;\/\u0026quot;):\n            file = blob.name\n            bucket = storage_client.get_bucket(bucket_name)\n        \n        #TODO: iterate the files into the path and parse using the new model\n        \n        logging.info(f\u0026quot;file: {file}\u0026quot;)\n\n        \n        with open(file, \u0026quot;w\u0026quot;) as j_file:\n                    j_file.write(json_util.dumps(file))\n                    j_file.write(\u0026quot;\\n\u0026quot;)\n        # mapper del modelo\n        new_model = EventsModel.parse_obj(j_file)\n        new_model = new_model.dict()\n\n        with open(new_model, \u0026quot;w\u0026quot;) as file_transformed:\n            file_transformed.write(json.dumps(new_model))\n            file_transformed.write(\u0026quot;\\n\u0026quot;)\n        \n        blob = bucket.blob(f\u0026quot;test_file\u0026quot;)\n        blob.upload_from_filename(file_transformed)\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EAdded it in DAG :-\u003C\/p\u003E\n\u003Cpre\u003E\u003Ccode\u003Emap_json_keys_with_BQ_columns = PythonOperator(\n        task_id=\u0026quot;map_json_keys_with_BQ_columns\u0026quot;,\n        retries=0,\n        python_callable=map_keys,\n        op_kwargs={\n            \u0026quot;bucket_name\u0026quot;: mcp_bucket,\n            \u0026quot;file_path\u0026quot;: mcp_source_folder,\n            \u0026quot;list_of_files\u0026quot;: source_files\n        },\n        dag=dag,\n    )\n\n\n    mcp_ingestion_to_bq = GCSToBigQueryOperator(\n        task_id=\u0026quot;mcp_ingestion_to_bq\u0026quot;,\n        retries=0,\n        dag=dag,\n        bucket=mcp_bucket,\n        source_objects=f\u0026quot;{mcp_source_folder}*.json\u0026quot;,\n        source_format=\u0026quot;NEWLINE_DELIMITED_JSON\u0026quot;,\n        #skip_leading_rows=16,\n        destination_project_dataset_table=destination_bq_table,\n        write_disposition=\u0026quot;WRITE_TRUNCATE\u0026quot;,\n        create_disposition=\u0026quot;CREATE_NEVER\u0026quot;,\n        \n        autodetect=True\n    )\n\u003C\/code\u003E\u003C\/pre\u003E\n\u003Cp\u003EBut it\u0027s not working. Getting an error :-\n\u003Ca href=\u0022https:\/\/i.stack.imgur.com\/KmZI4.png\u0022 rel=\u0022nofollow noreferrer\u0022\u003E\u003Cimg src=\u0022https:\/\/i.stack.imgur.com\/KmZI4.png\u0022 alt=\u0022enter image description here\u0022 \/\u003E\u003C\/a\u003E\nError :-\n\u003Ca href=\u0022https:\/\/i.stack.imgur.com\/RmK37.png\u0022 rel=\u0022nofollow noreferrer\u0022\u003E\u003Cimg src=\u0022https:\/\/i.stack.imgur.com\/RmK37.png\u0022 alt=\u0022enter image description here\u0022 \/\u003E\u003C\/a\u003E\u003C\/p\u003E","accepted_answer_id":null,"answer_count":0,"comment_count":0,"community_owned_date":null,"creation_date":{},"favorite_count":null,"last_activity_date":{},"last_edit_date":null,"last_editor_display_name":null,"last_editor_user_id":null,"owner_display_name":null,"owner_user_id":19234248,"parent_id":null,"post_type_id":1,"score":0,"tags":"python|google-cloud-platform|google-bigquery|airflow|google-cloud-composer","view_count":6}]')
            )
        ;

        $bigQueryService
            ->method('getPostComents')
            ->willReturn(
                json_decode( '[{"id":124906866,"text":"\u0022Sequential\u0022 and \u0022parallel\u0022 are opposed. Decide if you want to run them sequentially or parallel, and then do that. You can\u0027t do both.","creation_date":{},"post_id":70656333,"user_id":7426,"user_display_name":null,"score":1},{"id":124905389,"text":"If you need those function calls to execute in sequence, why run them concurrently?","creation_date":{},"post_id":70656333,"user_id":2541573,"user_display_name":null,"score":5}]')
            )
        ;

        return $bigQueryService;
    }
}
